{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**House Prices Prediction**\n",
        "###*Dataset - House Sales Prediction (Kaggle)*\n",
        "***Link to Dataset - https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data***\n",
        "\n"
      ],
      "metadata": {
        "id": "bdQtM5BgcP7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##***1. Setup and dataset loading***\n"
      ],
      "metadata": {
        "id": "w9vNSd1rcPu_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing Necessary Libraries"
      ],
      "metadata": {
        "id": "ZJ1XJATdbanl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import joblib\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "UIbXYacLJc2g"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Creating Functions to process data and perform Feature Engineering**"
      ],
      "metadata": {
        "id": "ICEEVt9hfBaG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*Processing train.csv to add features and remove unwanted items(like outliers) for model training*"
      ],
      "metadata": {
        "id": "yQYzMi9EvQT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HousePricePreprocessor:\n",
        "    def __init__(self):\n",
        "        self.label_encoders = {}\n",
        "        self.scaler = StandardScaler()\n",
        "        self.numerical_imputer = SimpleImputer(strategy='median')\n",
        "        self.categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "        self.numerical_cols = []\n",
        "        self.categorical_cols = []\n",
        "\n",
        "    def load_data(self, file_path):\n",
        "        \"\"\"Load the dataset\"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "            print(f\"Data loaded successfully! Shape: {df.shape}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data: {e}\")\n",
        "            return None\n",
        "\n",
        "    def basic_info(self, df):\n",
        "        \"\"\"Display basic information about the dataset\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"BASIC DATA INFORMATION\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"Dataset Shape: {df.shape}\")\n",
        "        print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "        print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
        "\n",
        "        # Missing values summary\n",
        "        missing = df.isnull().sum()\n",
        "        missing_percent = (missing / len(df)) * 100\n",
        "        missing_df = pd.DataFrame({\n",
        "            'Missing_Count': missing,\n",
        "            'Missing_Percent': missing_percent\n",
        "        }).sort_values('Missing_Count', ascending=False)\n",
        "\n",
        "        missing_features = missing_df[missing_df['Missing_Count'] > 0]\n",
        "        if len(missing_features) > 0:\n",
        "            print(f\"\\nFeatures with missing values: {len(missing_features)}\")\n",
        "            print(\"\\nTop 10 features with most missing values:\")\n",
        "            print(missing_features.head(10))\n",
        "        else:\n",
        "            print(\"\\nNo missing values found!\")\n",
        "\n",
        "        return missing_df\n",
        "\n",
        "    def identify_column_types(self, df):\n",
        "        \"\"\"Identify numerical and categorical columns\"\"\"\n",
        "        self.numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        self.categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "        # Remove target variable if present\n",
        "        if 'SalePrice' in self.numerical_cols:\n",
        "            self.numerical_cols.remove('SalePrice')\n",
        "\n",
        "        print(f\"\\nNumerical columns ({len(self.numerical_cols)}): {self.numerical_cols[:5]}...\")\n",
        "        print(f\"Categorical columns ({len(self.categorical_cols)}): {self.categorical_cols[:5]}...\")\n",
        "\n",
        "        return self.numerical_cols, self.categorical_cols\n",
        "\n",
        "    def handle_missing_values(self, df):\n",
        "        \"\"\"Handle missing values in the dataset\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"HANDLING MISSING VALUES\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        df_clean = df.copy()\n",
        "\n",
        "        # Special handling for specific columns based on domain knowledge\n",
        "        special_na_cols = {\n",
        "            'Alley': 'No_Alley',\n",
        "            'BsmtQual': 'No_Basement',\n",
        "            'BsmtCond': 'No_Basement',\n",
        "            'BsmtExposure': 'No_Basement',\n",
        "            'BsmtFinType1': 'No_Basement',\n",
        "            'BsmtFinType2': 'No_Basement',\n",
        "            'FireplaceQu': 'No_Fireplace',\n",
        "            'GarageType': 'No_Garage',\n",
        "            'GarageFinish': 'No_Garage',\n",
        "            'GarageQual': 'No_Garage',\n",
        "            'GarageCond': 'No_Garage',\n",
        "            'PoolQC': 'No_Pool',\n",
        "            'Fence': 'No_Fence',\n",
        "            'MiscFeature': 'None'\n",
        "        }\n",
        "\n",
        "        # Fill special NA columns\n",
        "        for col, fill_value in special_na_cols.items():\n",
        "            if col in df_clean.columns:\n",
        "                df_clean[col] = df_clean[col].fillna(fill_value)\n",
        "                print(f\" {col}: Filled {df[col].isnull().sum()} missing values with '{fill_value}'\")\n",
        "\n",
        "        # Handle LotFrontage (often missing, use median by neighborhood)\n",
        "        if 'LotFrontage' in df_clean.columns:\n",
        "            df_clean['LotFrontage'] = df_clean.groupby('Neighborhood')['LotFrontage'].transform(\n",
        "                lambda x: x.fillna(x.median())\n",
        "            )\n",
        "            print(\"LotFrontage: Filled with neighborhood median\")\n",
        "\n",
        "        # Handle GarageYrBlt\n",
        "        if 'GarageYrBlt' in df_clean.columns:\n",
        "            df_clean['GarageYrBlt'] = df_clean['GarageYrBlt'].fillna(df_clean['YearBuilt'])\n",
        "            print(\"GarageYrBlt: Filled with YearBuilt\")\n",
        "\n",
        "        # Handle MasVnrArea and MasVnrType\n",
        "        if 'MasVnrArea' in df_clean.columns:\n",
        "            df_clean['MasVnrArea'] = df_clean['MasVnrArea'].fillna(0)\n",
        "        if 'MasVnrType' in df_clean.columns:\n",
        "            df_clean['MasVnrType'] = df_clean['MasVnrType'].fillna('None')\n",
        "            print(\"MasVnr columns: Handled missing values\")\n",
        "\n",
        "        # Fill remaining numerical columns with median\n",
        "        remaining_num_cols = [col for col in self.numerical_cols if df_clean[col].isnull().sum() > 0]\n",
        "        if remaining_num_cols:\n",
        "            df_clean[remaining_num_cols] = self.numerical_imputer.fit_transform(df_clean[remaining_num_cols])\n",
        "            print(f\"Numerical columns: Filled {len(remaining_num_cols)} columns with median\")\n",
        "\n",
        "        # Fill remaining categorical columns with mode\n",
        "        remaining_cat_cols = [col for col in self.categorical_cols if df_clean[col].isnull().sum() > 0]\n",
        "        if remaining_cat_cols:\n",
        "            for col in remaining_cat_cols:\n",
        "                mode_value = df_clean[col].mode()[0] if not df_clean[col].mode().empty else 'Unknown'\n",
        "                df_clean[col] = df_clean[col].fillna(mode_value)\n",
        "            print(f\"Categorical columns: Filled {len(remaining_cat_cols)} columns with mode\")\n",
        "\n",
        "        print(f\"\\nMissing values after cleaning: {df_clean.isnull().sum().sum()}\")\n",
        "        return df_clean\n",
        "\n",
        "    def create_new_features(self, df):\n",
        "        \"\"\"Create new features from existing ones\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"FEATURE ENGINEERING\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        df_features = df.copy()\n",
        "\n",
        "        # Age features\n",
        "        if 'YearBuilt' in df_features.columns:\n",
        "            df_features['House_Age'] = 2023 - df_features['YearBuilt']\n",
        "            print(\"Created: House_Age\")\n",
        "\n",
        "        if 'YearRemodAdd' in df_features.columns:\n",
        "            df_features['Years_Since_Remod'] = 2023 - df_features['YearRemodAdd']\n",
        "            df_features['Was_Remodeled'] = (df_features['YearBuilt'] != df_features['YearRemodAdd']).astype(int)\n",
        "            print(\"Created: Years_Since_Remod, Was_Remodeled\")\n",
        "\n",
        "        # Total area features\n",
        "        if all(col in df_features.columns for col in ['1stFlrSF', '2ndFlrSF']):\n",
        "            df_features['Total_SF'] = df_features['1stFlrSF'] + df_features['2ndFlrSF']\n",
        "            print(\"Created: Total_SF\")\n",
        "\n",
        "        if 'TotalBsmtSF' in df_features.columns and 'Total_SF' in df_features.columns:\n",
        "            df_features['Total_Area'] = df_features['Total_SF'] + df_features['TotalBsmtSF']\n",
        "            print(\"Created: Total_Area\")\n",
        "\n",
        "        # Bathroom features\n",
        "        bathroom_cols = ['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath']\n",
        "        if all(col in df_features.columns for col in bathroom_cols):\n",
        "            df_features['Total_Bathrooms'] = (df_features['FullBath'] +\n",
        "                                            df_features['BsmtFullBath'] +\n",
        "                                            0.5 * df_features['HalfBath'] +\n",
        "                                            0.5 * df_features['BsmtHalfBath'])\n",
        "            print(\"Created: Total_Bathrooms\")\n",
        "\n",
        "        # Porch features\n",
        "        porch_cols = ['OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch']\n",
        "        available_porch_cols = [col for col in porch_cols if col in df_features.columns]\n",
        "        if available_porch_cols:\n",
        "            df_features['Total_Porch_SF'] = df_features[available_porch_cols].sum(axis=1)\n",
        "            print(\"Created: Total_Porch_SF\")\n",
        "\n",
        "        # Quality features\n",
        "        quality_cols = ['OverallQual', 'OverallCond']\n",
        "        if all(col in df_features.columns for col in quality_cols):\n",
        "            df_features['Overall_Quality_Score'] = df_features['OverallQual'] * df_features['OverallCond']\n",
        "            print(\"Created: Overall_Quality_Score\")\n",
        "\n",
        "        # Garage features\n",
        "        if 'GarageCars' in df_features.columns:\n",
        "            df_features['Has_Garage'] = (df_features['GarageCars'] > 0).astype(int)\n",
        "            print(\"Created: Has_Garage\")\n",
        "\n",
        "        # Basement features\n",
        "        if 'TotalBsmtSF' in df_features.columns:\n",
        "            df_features['Has_Basement'] = (df_features['TotalBsmtSF'] > 0).astype(int)\n",
        "            print(\"Created: Has_Basement\")\n",
        "\n",
        "        print(f\"\\nNew dataset shape: {df_features.shape}\")\n",
        "        return df_features\n",
        "\n",
        "    def encode_categorical_features(self, df):\n",
        "        \"\"\"Encode categorical features\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"ENCODING CATEGORICAL FEATURES\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        df_encoded = df.copy()\n",
        "\n",
        "        # Update categorical columns list\n",
        "        categorical_cols = df_encoded.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "        # Ordinal encoding for quality/condition columns\n",
        "        ordinal_mappings = {\n",
        "            'ExterQual': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
        "            'ExterCond': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
        "            'BsmtQual': {'No_Basement': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
        "            'BsmtCond': {'No_Basement': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
        "            'HeatingQC': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
        "            'KitchenQual': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
        "            'FireplaceQu': {'No_Fireplace': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
        "            'GarageQual': {'No_Garage': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
        "            'GarageCond': {'No_Garage': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
        "        }\n",
        "\n",
        "        # Apply ordinal encoding\n",
        "        ordinal_encoded = []\n",
        "        for col, mapping in ordinal_mappings.items():\n",
        "            if col in df_encoded.columns:\n",
        "                df_encoded[col] = df_encoded[col].map(mapping)\n",
        "                ordinal_encoded.append(col)\n",
        "                categorical_cols.remove(col)\n",
        "\n",
        "        print(f\"Ordinal encoded: {len(ordinal_encoded)} columns\")\n",
        "\n",
        "        # Label encoding for remaining categorical columns\n",
        "        label_encoded = []\n",
        "        for col in categorical_cols:\n",
        "            if col in df_encoded.columns:\n",
        "                le = LabelEncoder()\n",
        "                df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n",
        "                self.label_encoders[col] = le\n",
        "                label_encoded.append(col)\n",
        "\n",
        "        print(f\"Label encoded: {len(label_encoded)} columns\")\n",
        "        print(f\"Total categorical features processed: {len(ordinal_encoded) + len(label_encoded)}\")\n",
        "\n",
        "        return df_encoded\n",
        "\n",
        "    def scale_features(self, df, target_col='SalePrice'):\n",
        "        \"\"\"Scale numerical features\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"SCALING FEATURES\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        df_scaled = df.copy()\n",
        "\n",
        "        # Get numerical columns (excluding target)\n",
        "        numerical_cols = df_scaled.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        if target_col in numerical_cols:\n",
        "            numerical_cols.remove(target_col)\n",
        "\n",
        "        # Scale numerical features\n",
        "        df_scaled[numerical_cols] = self.scaler.fit_transform(df_scaled[numerical_cols])\n",
        "\n",
        "        print(f\"Scaled {len(numerical_cols)} numerical features\")\n",
        "        print(\"Scaling method: StandardScaler (mean=0, std=1)\")\n",
        "\n",
        "        return df_scaled\n",
        "\n",
        "    def preprocess_features(self, df):\n",
        "      \"\"\"\n",
        "      Preprocess features for prediction (used by Streamlit)\n",
        "      This method handles single prediction inputs\n",
        "      \"\"\"\n",
        "      print(\"\\n\" + \"=\"*50)\n",
        "      print(\"PREPROCESSING FEATURES FOR PREDICTION\")\n",
        "      print(\"=\"*50)\n",
        "\n",
        "      df_processed = df.copy()\n",
        "\n",
        "      # Convert Yes/No to 1/0 for categorical columns\n",
        "      yes_no_cols = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
        "      for col in yes_no_cols:\n",
        "        if col in df_processed.columns:\n",
        "          df_processed[col] = df_processed[col].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "      # Create basic feature engineering (simplified version)\n",
        "      # Age feature (assuming current year is 2023)\n",
        "      if 'yearbuilt' in df_processed.columns:\n",
        "        df_processed['House_Age'] = 2023 - df_processed['yearbuilt']\n",
        "\n",
        "      # Total bathrooms (simplified)\n",
        "      if 'bathrooms' in df_processed.columns:\n",
        "        df_processed['Total_Bathrooms'] = df_processed['bathrooms']\n",
        "\n",
        "      # Has garage\n",
        "      if 'parking' in df_processed.columns:\n",
        "        df_processed['Has_Garage'] = (df_processed['parking'] > 0).astype(int)\n",
        "\n",
        "      # Has basement\n",
        "      if 'basement' in df_processed.columns:\n",
        "        df_processed['Has_Basement'] = df_processed['basement']\n",
        "\n",
        "      # Quality score (simplified)\n",
        "      if 'bedrooms' in df_processed.columns and 'bathrooms' in df_processed.columns:\n",
        "        df_processed['Overall_Quality_Score'] = df_processed['bedrooms'] * df_processed['bathrooms']\n",
        "\n",
        "      print(f\"Processed features shape: {df_processed.shape}\")\n",
        "      return df_processed\n",
        "\n",
        "    def remove_outliers(self, df, target_col='SalePrice'):\n",
        "        \"\"\"Remove outliers using IQR method\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"REMOVING OUTLIERS\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        df_clean = df.copy()\n",
        "        initial_shape = df_clean.shape[0]\n",
        "\n",
        "        # Remove outliers from target variable if present\n",
        "        if target_col in df_clean.columns:\n",
        "            Q1 = df_clean[target_col].quantile(0.25)\n",
        "            Q3 = df_clean[target_col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "            df_clean = df_clean[(df_clean[target_col] >= lower_bound) &\n",
        "                               (df_clean[target_col] <= upper_bound)]\n",
        "\n",
        "            removed = initial_shape - df_clean.shape[0]\n",
        "            print(f\"Removed {removed} outliers from {target_col}\")\n",
        "            print(f\"Remaining samples: {df_clean.shape[0]} ({100*df_clean.shape[0]/initial_shape:.1f}%)\")\n",
        "\n",
        "        return df_clean\n",
        "\n",
        "    def get_feature_importance_data(self, df, target_col='SalePrice'):\n",
        "        \"\"\"Prepare data for feature importance analysis\"\"\"\n",
        "        if target_col not in df.columns:\n",
        "            print(f\"Target column '{target_col}' not found\")\n",
        "            return None, None\n",
        "\n",
        "        X = df.drop(columns=[target_col])\n",
        "        y = df[target_col]\n",
        "\n",
        "        print(f\"Prepared data for modeling: X{X.shape}, y{y.shape}\")\n",
        "        return X, y\n",
        "\n",
        "    def full_preprocessing_pipeline(self, file_path, remove_outliers=True):\n",
        "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
        "        print(\"HOUSE PRICES DATA PREPROCESSING PIPELINE\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Step 1: Load data\n",
        "        df = self.load_data(file_path)\n",
        "        if df is None:\n",
        "            return None\n",
        "\n",
        "        # Step 2: Basic info\n",
        "        self.basic_info(df)\n",
        "\n",
        "        # Step 3: Identify column types\n",
        "        self.identify_column_types(df)\n",
        "\n",
        "        # Step 4: Handle missing values\n",
        "        df = self.handle_missing_values(df)\n",
        "\n",
        "        # Step 5: Feature engineering\n",
        "        df = self.create_new_features(df)\n",
        "\n",
        "        # Step 6: Encode categorical features\n",
        "        df = self.encode_categorical_features(df)\n",
        "\n",
        "        # Step 7: Remove outliers (optional)\n",
        "        if remove_outliers:\n",
        "            df = self.remove_outliers(df)\n",
        "\n",
        "        # Step 8: Scale features\n",
        "        df = self.scale_features(df)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"PREPROCESSING COMPLETED SUCCESSFULLY!\")\n",
        "        print(f\"Final dataset shape: {df.shape}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        return df"
      ],
      "metadata": {
        "id": "DzYSgKKlKRYM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Processing test.csv and training model on train.csv to predict house prices**"
      ],
      "metadata": {
        "id": "pX79-NWtfSTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*Using Different models(Random Forest, Gradient Boosting) to analyse which model predits more accurately*"
      ],
      "metadata": {
        "id": "DxEe7Dlov5V9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_predict(preprocessor, processed_train_df, test_file_path):\n",
        "    \"\"\"\n",
        "    Simple function to train model and make predictions\n",
        "    Uses already processed training data to avoid duplicate preprocessing\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TRAINING MODEL AND MAKING PREDICTIONS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 1. Use already processed training data\n",
        "    print(\"Using pre-processed training data...\")\n",
        "    train_df = processed_train_df\n",
        "\n",
        "    if train_df is None:\n",
        "        print(\"No processed training data provided\")\n",
        "        return None\n",
        "\n",
        "    # 2. Prepare training data\n",
        "    X, y = preprocessor.get_feature_importance_data(train_df, 'SalePrice')\n",
        "\n",
        "    if X is None:\n",
        "        print(\"No target variable found\")\n",
        "        return None\n",
        "\n",
        "    # 3. Split data for validation\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    print(f\"Data split - Train: {X_train.shape}, Validation: {X_val.shape}\")\n",
        "\n",
        "    # 4. Train models and select best one\n",
        "    models = {\n",
        "        'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1),\n",
        "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
        "    }\n",
        "\n",
        "    best_model = None\n",
        "    best_score = float('inf')\n",
        "    best_name = \"\"\n",
        "\n",
        "    print(\"\\nTraining models...\")\n",
        "    for name, model in models.items():\n",
        "        # Train model\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Validate\n",
        "        val_pred = model.predict(X_val)\n",
        "        rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
        "        r2 = r2_score(y_val, val_pred)\n",
        "\n",
        "        print(f\"   {name}: RMSE=${rmse:,.0f}, R²={r2:.4f}\")\n",
        "\n",
        "        if rmse < best_score:\n",
        "            best_score = rmse\n",
        "            best_model = model\n",
        "            best_name = name\n",
        "\n",
        "    print(f\"\\nBest model: {best_name} (RMSE: ${best_score:,.0f})\")\n",
        "\n",
        "    # SAVE THE BEST MODEL (not just 'model')\n",
        "    joblib.dump(best_model, 'house_price_model.pkl')\n",
        "\n",
        "    # SAVE FEATURE NAMES AND SCALER FOR STREAMLIT\n",
        "    save_feature_names_and_scaler(preprocessor, X_train)\n",
        "\n",
        "    # 5. Process test data\n",
        "    print(\"\\nProcessing test data...\")\n",
        "    test_df = preprocessor.load_data(test_file_path)\n",
        "\n",
        "    if test_df is None:\n",
        "        print(\"Failed to load test data\")\n",
        "        return None\n",
        "\n",
        "    # Store test IDs if they exist\n",
        "    test_ids = None\n",
        "    if 'Id' in test_df.columns:\n",
        "        test_ids = test_df['Id'].copy()\n",
        "        test_df = test_df.drop('Id', axis=1)\n",
        "\n",
        "    # Apply same preprocessing to test data (without target operations)\n",
        "    original_numerical_cols = preprocessor.numerical_cols.copy()\n",
        "    original_categorical_cols = preprocessor.categorical_cols.copy()\n",
        "\n",
        "    # Identify columns\n",
        "    preprocessor.identify_column_types(test_df)\n",
        "\n",
        "    # Handle missing values\n",
        "    test_df = preprocessor.handle_missing_values(test_df)\n",
        "\n",
        "    # Create new features\n",
        "    test_df = preprocessor.create_new_features(test_df)\n",
        "\n",
        "    # Encode categorical features\n",
        "    test_df = preprocessor.encode_categorical_features(test_df)\n",
        "\n",
        "    # Align test data with training data features\n",
        "    train_features = X.columns.tolist()\n",
        "\n",
        "    # Add missing columns with zeros\n",
        "    for col in train_features:\n",
        "        if col not in test_df.columns:\n",
        "            test_df[col] = 0\n",
        "            print(f\"Added missing column: {col}\")\n",
        "\n",
        "    # Remove extra columns\n",
        "    extra_cols = [col for col in test_df.columns if col not in train_features]\n",
        "    if extra_cols:\n",
        "        test_df = test_df.drop(columns=extra_cols)\n",
        "        print(f\"Removed extra columns: {extra_cols}\")\n",
        "\n",
        "    # Reorder columns to match training data\n",
        "    test_df = test_df[train_features]\n",
        "\n",
        "    # Scale test data using the same scaler fitted on training data\n",
        "    numerical_cols = test_df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    test_df[numerical_cols] = preprocessor.scaler.transform(test_df[numerical_cols])\n",
        "\n",
        "    print(f\"Test data processed: {test_df.shape}\")\n",
        "\n",
        "    # 6. Make predictions\n",
        "    print(\"\\nMaking predictions...\")\n",
        "    predictions = best_model.predict(test_df)\n",
        "\n",
        "    # 7. Create submission file\n",
        "    if test_ids is not None:\n",
        "        submission = pd.DataFrame({\n",
        "            'Id': test_ids,\n",
        "            'SalePrice': predictions\n",
        "        })\n",
        "    else:\n",
        "        submission = pd.DataFrame({\n",
        "            'SalePrice': predictions\n",
        "        })\n",
        "\n",
        "    # Save predictions\n",
        "    submission.to_csv('house_price_predictions.csv', index=False)\n",
        "\n",
        "    print(\"Predictions saved to 'house_price_predictions.csv'\")\n",
        "    print(f\"Prediction stats:\")\n",
        "    print(f\"   Mean: ${predictions.mean():,.0f}\")\n",
        "    print(f\"   Median: ${np.median(predictions):,.0f}\")\n",
        "    print(f\"   Min: ${predictions.min():,.0f}\")\n",
        "    print(f\"   Max: ${predictions.max():,.0f}\")\n",
        "\n",
        "    # 8. Show feature importance\n",
        "    if hasattr(best_model, 'feature_importances_'):\n",
        "        print(f\"\\nTOP 10 MOST IMPORTANT FEATURES:\")\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'Feature': train_features,\n",
        "            'Importance': best_model.feature_importances_\n",
        "        }).sort_values('Importance', ascending=False)\n",
        "\n",
        "        for i, row in feature_importance.head(10).iterrows():\n",
        "            print(f\"   {row['Feature']:<20} {row['Importance']:.4f}\")\n",
        "\n",
        "    print(\"\\nFiles created for Streamlit:\")\n",
        "    print(\"- house_price_model.pkl (trained model)\")\n",
        "    print(\"- feature_names.pkl (feature names)\")\n",
        "    print(\"- scaler.pkl (fitted scaler)\")\n",
        "\n",
        "    return submission, best_model\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "U4aI9uGqeu8z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Saving Feature names for appropriate scaling**"
      ],
      "metadata": {
        "id": "mfsXsggQpQP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_feature_names_and_scaler(preprocessor, X_train):\n",
        "    \"\"\"\n",
        "    Save feature names and scaler for later use in Streamlit\n",
        "    \"\"\"\n",
        "    import pickle\n",
        "\n",
        "    # Save feature names\n",
        "    with open('feature_names.pkl', 'wb') as f:\n",
        "        pickle.dump(X_train.columns.tolist(), f)\n",
        "\n",
        "    # Save the fitted scaler\n",
        "    with open('scaler.pkl', 'wb') as f:\n",
        "        pickle.dump(preprocessor.scaler, f)\n",
        "\n",
        "    print(f\"Saved {len(X_train.columns)} feature names and scaler\")\n"
      ],
      "metadata": {
        "id": "PbwrPt6jA0lq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**processed_house_data.csv and house_price_predictions.csv created using Random Forest**"
      ],
      "metadata": {
        "id": "2zUL1mGbwPA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    preprocessor = HousePricePreprocessor()\n",
        "\n",
        "    #from google.colab import drive\n",
        "    #drive.mount('/content/drive')\n",
        "\n",
        "    # File paths\n",
        "    train_file_path = '/content/drive/MyDrive/train.csv'\n",
        "    test_file_path = '/content/drive/MyDrive/test.csv'\n",
        "\n",
        "    processed_df = preprocessor.full_preprocessing_pipeline(train_file_path)\n",
        "\n",
        "    if processed_df is not None:\n",
        "        processed_df.to_csv('processed_house_data.csv', index=False)\n",
        "        print(\"Processed data saved as 'processed_house_data.csv'\")\n",
        "\n",
        "        X, y = preprocessor.get_feature_importance_data(processed_df)\n",
        "        if X is not None:\n",
        "            print(f\"\\nReady for modeling with {X.shape[1]} features!\")\n",
        "\n",
        "        predictions, model = train_and_predict(preprocessor, processed_df, test_file_path)\n",
        "\n",
        "        if predictions is not None:\n",
        "            print(\"\\n house_price_predictions.csv created.\")\n",
        "            print(\"Predictions:\")\n",
        "            print(predictions.head())\n",
        "        else:\n",
        "            print(\"Something went wrong. Please check your file paths and data.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RADvgpapgZqM",
        "outputId": "2e8978e0-fd9f-4868-ff98-1b6651a984df"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HOUSE PRICES DATA PREPROCESSING PIPELINE\n",
            "============================================================\n",
            "Data loaded successfully! Shape: (1460, 81)\n",
            "\n",
            "==================================================\n",
            "BASIC DATA INFORMATION\n",
            "==================================================\n",
            "Dataset Shape: (1460, 81)\n",
            "Memory Usage: 3.86 MB\n",
            "Duplicate rows: 0\n",
            "\n",
            "Features with missing values: 19\n",
            "\n",
            "Top 10 features with most missing values:\n",
            "              Missing_Count  Missing_Percent\n",
            "PoolQC                 1453        99.520548\n",
            "MiscFeature            1406        96.301370\n",
            "Alley                  1369        93.767123\n",
            "Fence                  1179        80.753425\n",
            "MasVnrType              872        59.726027\n",
            "FireplaceQu             690        47.260274\n",
            "LotFrontage             259        17.739726\n",
            "GarageQual               81         5.547945\n",
            "GarageFinish             81         5.547945\n",
            "GarageType               81         5.547945\n",
            "\n",
            "Numerical columns (37): ['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual']...\n",
            "Categorical columns (43): ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour']...\n",
            "\n",
            "==================================================\n",
            "HANDLING MISSING VALUES\n",
            "==================================================\n",
            " Alley: Filled 1369 missing values with 'No_Alley'\n",
            " BsmtQual: Filled 37 missing values with 'No_Basement'\n",
            " BsmtCond: Filled 37 missing values with 'No_Basement'\n",
            " BsmtExposure: Filled 38 missing values with 'No_Basement'\n",
            " BsmtFinType1: Filled 37 missing values with 'No_Basement'\n",
            " BsmtFinType2: Filled 38 missing values with 'No_Basement'\n",
            " FireplaceQu: Filled 690 missing values with 'No_Fireplace'\n",
            " GarageType: Filled 81 missing values with 'No_Garage'\n",
            " GarageFinish: Filled 81 missing values with 'No_Garage'\n",
            " GarageQual: Filled 81 missing values with 'No_Garage'\n",
            " GarageCond: Filled 81 missing values with 'No_Garage'\n",
            " PoolQC: Filled 1453 missing values with 'No_Pool'\n",
            " Fence: Filled 1179 missing values with 'No_Fence'\n",
            " MiscFeature: Filled 1406 missing values with 'None'\n",
            "LotFrontage: Filled with neighborhood median\n",
            "GarageYrBlt: Filled with YearBuilt\n",
            "MasVnr columns: Handled missing values\n",
            "Categorical columns: Filled 1 columns with mode\n",
            "\n",
            "Missing values after cleaning: 0\n",
            "\n",
            "==================================================\n",
            "FEATURE ENGINEERING\n",
            "==================================================\n",
            "Created: House_Age\n",
            "Created: Years_Since_Remod, Was_Remodeled\n",
            "Created: Total_SF\n",
            "Created: Total_Area\n",
            "Created: Total_Bathrooms\n",
            "Created: Total_Porch_SF\n",
            "Created: Overall_Quality_Score\n",
            "Created: Has_Garage\n",
            "Created: Has_Basement\n",
            "\n",
            "New dataset shape: (1460, 91)\n",
            "\n",
            "==================================================\n",
            "ENCODING CATEGORICAL FEATURES\n",
            "==================================================\n",
            "Ordinal encoded: 9 columns\n",
            "Label encoded: 34 columns\n",
            "Total categorical features processed: 43\n",
            "\n",
            "==================================================\n",
            "REMOVING OUTLIERS\n",
            "==================================================\n",
            "Removed 61 outliers from SalePrice\n",
            "Remaining samples: 1399 (95.8%)\n",
            "\n",
            "==================================================\n",
            "SCALING FEATURES\n",
            "==================================================\n",
            "Scaled 90 numerical features\n",
            "Scaling method: StandardScaler (mean=0, std=1)\n",
            "\n",
            "============================================================\n",
            "PREPROCESSING COMPLETED SUCCESSFULLY!\n",
            "Final dataset shape: (1399, 91)\n",
            "============================================================\n",
            "Processed data saved as 'processed_house_data.csv'\n",
            "Prepared data for modeling: X(1399, 90), y(1399,)\n",
            "\n",
            "Ready for modeling with 90 features!\n",
            "\n",
            "============================================================\n",
            "TRAINING MODEL AND MAKING PREDICTIONS\n",
            "============================================================\n",
            "Using pre-processed training data...\n",
            "Prepared data for modeling: X(1399, 90), y(1399,)\n",
            "Data split - Train: (1119, 90), Validation: (280, 90)\n",
            "\n",
            "Training models...\n",
            "   Random Forest: RMSE=$20,022, R²=0.8740\n",
            "   Gradient Boosting: RMSE=$20,090, R²=0.8731\n",
            "\n",
            "Best model: Random Forest (RMSE: $20,022)\n",
            "Saved 90 feature names and scaler\n",
            "\n",
            "Processing test data...\n",
            "Data loaded successfully! Shape: (1459, 80)\n",
            "\n",
            "Numerical columns (36): ['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond']...\n",
            "Categorical columns (43): ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour']...\n",
            "\n",
            "==================================================\n",
            "HANDLING MISSING VALUES\n",
            "==================================================\n",
            " Alley: Filled 1352 missing values with 'No_Alley'\n",
            " BsmtQual: Filled 44 missing values with 'No_Basement'\n",
            " BsmtCond: Filled 45 missing values with 'No_Basement'\n",
            " BsmtExposure: Filled 44 missing values with 'No_Basement'\n",
            " BsmtFinType1: Filled 42 missing values with 'No_Basement'\n",
            " BsmtFinType2: Filled 42 missing values with 'No_Basement'\n",
            " FireplaceQu: Filled 730 missing values with 'No_Fireplace'\n",
            " GarageType: Filled 76 missing values with 'No_Garage'\n",
            " GarageFinish: Filled 78 missing values with 'No_Garage'\n",
            " GarageQual: Filled 78 missing values with 'No_Garage'\n",
            " GarageCond: Filled 78 missing values with 'No_Garage'\n",
            " PoolQC: Filled 1456 missing values with 'No_Pool'\n",
            " Fence: Filled 1169 missing values with 'No_Fence'\n",
            " MiscFeature: Filled 1408 missing values with 'None'\n",
            "LotFrontage: Filled with neighborhood median\n",
            "GarageYrBlt: Filled with YearBuilt\n",
            "MasVnr columns: Handled missing values\n",
            "Numerical columns: Filled 8 columns with median\n",
            "Categorical columns: Filled 7 columns with mode\n",
            "\n",
            "Missing values after cleaning: 0\n",
            "\n",
            "==================================================\n",
            "FEATURE ENGINEERING\n",
            "==================================================\n",
            "Created: House_Age\n",
            "Created: Years_Since_Remod, Was_Remodeled\n",
            "Created: Total_SF\n",
            "Created: Total_Area\n",
            "Created: Total_Bathrooms\n",
            "Created: Total_Porch_SF\n",
            "Created: Overall_Quality_Score\n",
            "Created: Has_Garage\n",
            "Created: Has_Basement\n",
            "\n",
            "New dataset shape: (1459, 89)\n",
            "\n",
            "==================================================\n",
            "ENCODING CATEGORICAL FEATURES\n",
            "==================================================\n",
            "Ordinal encoded: 9 columns\n",
            "Label encoded: 34 columns\n",
            "Total categorical features processed: 43\n",
            "Added missing column: Id\n",
            "Test data processed: (1459, 90)\n",
            "\n",
            "Making predictions...\n",
            "Predictions saved to 'house_price_predictions.csv'\n",
            "Prediction stats:\n",
            "   Mean: $173,170\n",
            "   Median: $159,633\n",
            "   Min: $57,856\n",
            "   Max: $315,098\n",
            "\n",
            "TOP 10 MOST IMPORTANT FEATURES:\n",
            "   Total_Area           0.4258\n",
            "   OverallQual          0.3020\n",
            "   Overall_Quality_Score 0.0201\n",
            "   Total_Bathrooms      0.0173\n",
            "   KitchenQual          0.0169\n",
            "   GarageArea           0.0133\n",
            "   LotArea              0.0127\n",
            "   ExterQual            0.0106\n",
            "   House_Age            0.0101\n",
            "   BsmtUnfSF            0.0095\n",
            "\n",
            "Files created for Streamlit:\n",
            "- house_price_model.pkl (trained model)\n",
            "- feature_names.pkl (feature names)\n",
            "- scaler.pkl (fitted scaler)\n",
            "\n",
            " house_price_predictions.csv created.\n",
            "Predictions:\n",
            "     Id      SalePrice\n",
            "0  1461  127879.197869\n",
            "1  1462  158334.351959\n",
            "2  1463  177146.031968\n",
            "3  1464  182530.071300\n",
            "4  1465  199781.759389\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Steps involved:\n",
        "- Cleaned and prepared the training dataset (`train.csv`) by removing outliers and imputing missing values.\n",
        "- Engineered relevant features for better model performance.\n",
        "- Processed the test dataset (`test.csv`) in a similar fashion to ensure consistent input.\n",
        "- Test data after preprocessing is also saved in `processed_house_data.csv` for reference.\n",
        "- Trained multiple models including Random Forest and Gradient Boosting Regressors.\n",
        "- Selected Random Forest as the final model based on its superior performance.\n",
        "- Saved the trained model (`house_price_model.pkl`), feature names (`feature_names.pkl`), and scaler (`scaler.pkl`) for deployment.\n",
        "- Generated predictions on the test set and saved them in `house_price_predictions.csv`."
      ],
      "metadata": {
        "id": "t4lK2cBu0mCl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JXC2O0Jy2kZF"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}